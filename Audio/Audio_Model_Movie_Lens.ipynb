{"cells":[{"cell_type":"markdown","metadata":{"id":"eoP5Mjtyzp3P"},"source":["# Helpers"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WApHUHGBxZTM"},"outputs":[],"source":["!pip install pickle5"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"y0Hd9fVtxfSP"},"outputs":[],"source":["!pip install keras_applications"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SYwjSXSlKxPs"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hiNOmIuByOv-"},"outputs":[],"source":["import os\n","import pickle5 as pickle\n","from pathlib import Path\n","import random\n","import tensorflow as tf\n","import keras\n","import string\n","from math import ceil\n","import random\n","import numpy as np\n","import sys\n","import os\n","from PIL import Image\n","from tqdm import tqdm\n","from sklearn.metrics import mean_squared_error as mse"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VvCHHB1YzkXa"},"outputs":[],"source":["def path_of(location):\n","    me_dir, me_file= os.path.split(os.path.abspath(__file__))\n","    return os.path.join(me_dir, location)\n","\n","def load_pkl(filename):\n","    filename= filename\n","    data= None\n","    with open(filename, \"rb\") as handle:\n","        data= pickle.load(handle)\n","        handle.close()\n","    return data\n","\n","def store_pkl(object, filename):\n","    filename= filename\n","    with open(filename, \"wb\") as handle:\n","        pickle.dump(object, handle, protocol=pickle.HIGHEST_PROTOCOL)\n","        handle.close()\n","\n","def is_valid_file(filename):\n","    filename= path_of(filename)\n","    file= Path(filename)\n","    if file.is_file():\n","        return True\n","    return False\n","\n","\n","def to_same_shape(arr_of_items, required_shape):\n","    if len(arr_of_items) == 0:\n","        print(\"Error: tried to make shape: \", required_shape, \", but item is empty...\")\n","        exit()\n","    if len(arr_of_items)>required_shape:\n","        return arr_of_items[:required_shape]\n","    res= []\n","    ind=0\n","    while len(res)<required_shape:\n","        res.append(arr_of_items[ind])\n","        ind= (ind+1)%len(arr_of_items)\n","    return res\n","\n","def get_rand_str(size):\n","    return \"\".join(random.choice(string.ascii_uppercase + string.digits) for _ in range(size))\n","\n","def save_model(keras_model, save_folder=\"./models\", save_filename= None):\n","    if save_filename is None:\n","        me_dir, me_file= os.path.split(os.path.abspath(__file__))\n","        save_filename= me_file.split(\".\")[0]+\".h5\"\n","    op_file= path_of(save_folder +\"/\"+save_filename)\n","    keras_model.save(op_file)\n","    print(\"\\n\\n    Saved_model:\", save_filename, \"\\n\\n\")"]},{"cell_type":"markdown","metadata":{"id":"NASBYk9BzuTN"},"source":["# Model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Uiw6VKHNJ4hU"},"outputs":[],"source":["def save(self, location= \"./model_vid/\"):\n","    self.model_cnn.save(location+\"/model_cnn.h5\")\n","    self.model_user.save(location+\"/model_user.h5\")\n","    self.model_full.save(location+\"/model_full.h5\")"]},{"cell_type":"markdown","metadata":{"id":"YhGlHmdpz5TZ"},"source":["# Train"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1t5DdjG70AIx"},"outputs":[],"source":["use_precomputed_dict_for_video= False\n","\n","\n","input_dimension = 88\n","audio_encode_dim = 88\n","\n","# imageW, imageH= 112, 112\n","\n","batch_size= 64\n","\n","train_obj= load_pkl(\"/content/drive/MyDrive/Recommendation_system/train_v2.obj\") #will load pkl data using func\n","test_obj= load_pkl(\"/content/drive/MyDrive/Recommendation_system/test_v2.obj\")   #will load pkl data using func\n","\n","\n","user_enc= load_pkl(\"/content/drive/MyDrive/Recommendation_system/new_user_embedding_v2_510.obj\")     #will load pkl data using func\n","user_size= len(user_enc.get(list(user_enc.keys())[0])) #user_size --> 915\n","#print(user_size)\n","\n","audio = load_pkl(\"/content/drive/MyDrive/Recommendation_system/audio_embedding_510.obj\")\n","\n","\n","def shuffle_single_epoch(ratings):  #will shuffle the provided data\n","    data_copied= ratings.copy() \n","    random.shuffle(data_copied)\n","    return data_copied\n","\n","    \n","def normalize_rate(rate):\n","    return rate/5\n","\n","def de_normalize_rate(rate):\n","    return rate*5\n","\n","\n","def get_nth_batch(data):\n","\n","    users, movies, nrates= [], [], [] # 3 empty list \n","    \n","    for user_id, movie_id, rate in data:\n","        #user_id --> 11megha89 , movie_id --.tt014651 , rate --> 1,0,-1\n","        if user_enc.get(user_id) is None:    #Return user_vec --> 915\n","            continue\n","        try:\n","            m_vid= audio[movie_id]\n","            #print(m_vid.shape) -->(112, 112, 9)\n","        except:\n","            continue\n","        users.append(user_enc.get(user_id)) #Will \n","        movies.append(m_vid)\n","        if int(rate) == -1:\n","               nrates.append([0,0,1])\n","        elif int(rate) == 1:\n","               nrates.append([0,1,0])\n","        elif int(rate) == 0:\n","               nrates.append([1,0,0])\n","               \n","    users= np.array(users, dtype=float)\n","    # print(users.shape) --> (64, 915)\n","    movies= np.array(movies, dtype=float)\n","    # print(movies.shape) --> (64, 112, 112, 9)\n","    nrates= np.array(nrates, dtype=float)\n","    #print(nrates.shape) --> (64,)\n","\n","    assert len(users)==len(movies)==len(nrates)\n","\n","    return users, movies, nrates\n","\n","\n","\n","def test(model, test_data):\n","    test_acc,test_loss= [],[]\n","    test_final_acc,test_final_loss = [] , []\n","    batch_count= ceil(len(test_data)/batch_size)\n","    for batch_id in range(batch_count):\n","        print(\" -> Testing Batch: \", batch_id)\n","        user, movie, rate= get_nth_batch(test_data, batch_id)\n","        loss , acc = model.evaluate([movie, user], rate)\n","        test_acc.append(acc)\n","        test_loss.append(loss)\n","        \n","    test_final_acc.append(sum(test_acc)/len(test_acc))\n","    test_final_loss.append(sum(test_loss)/len(test_acc))\n","    return sum(test_acc)/len(test_acc) , sum(test_loss)/len(test_acc)\n","\n","\n","\n","def train(model, data, test_data= None, epochs=50):\n","    test_acc_main = 0\n","    batch_count= ceil(len(data)/batch_size)       \n","    for epoch_id in range(1, epochs+1):\n","        data= shuffle_single_epoch(data)\n","        print(\"\\n\\t---- Starting Epoch:\", epoch_id, \"----\")\n","        \n","        for batch_id in range(batch_count):\n","            print(\" -> Batch: \", batch_id)\n","            print(batch_id)\n","            user, movie, rate= get_nth_batch(data, batch_id,batch_size) \n","            #print(rate.shape)                        \n","            model.fit([movie, user], rate, batch_size=batch_size, epochs=1)            \n","            \n","        if test_data is not None:\n","            test_acc , test_loss= test(model, test_data)\n","            print(\"TestAcc after Epoch\",epoch_id,\": \",test_acc)\n","            print(\"TestLoss after Epoch\",epoch_id,\": \",test_loss)\n","            if test_acc>test_acc_main:\n","                    test_acc_main = test_acc\n","                    location= \"./model_vid/\"\n","                    model.save(location+\"/model_abc.h5\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Og-cBaD7J4hZ"},"outputs":[],"source":["import pandas as pd\n","data_obj = test_obj + train_obj\n","pd_data = pd.DataFrame(data_obj)\n","\n","results = []\n","for i in pd_data.iloc[:,2]:\n","    results.append(i[0])\n","\n","pd_data[3] = results\n","x_data = np.array(pd_data.iloc[:,:2])\n","y_data = np.array(pd_data.iloc[:,3])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Mjpo8nh9J4ha"},"outputs":[],"source":["from sklearn.model_selection import train_test_split\n","\n","X_train, X_test, y_train, y_test = train_test_split(x_data, y_data, stratify=y_data ,test_size=0.2)\n","\n","train_obj = pd.DataFrame(X_train)\n","train_obj[3] = pd.DataFrame(y_train)\n","train_obj = np.array(train_obj)\n","\n","test_obj = pd.DataFrame(X_test)\n","test_obj[3] = pd.DataFrame(y_test)\n","test_obj = np.array(test_obj)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iCmUL-dJKMrQ"},"outputs":[],"source":["def audio_Main_Model(input_dim,audio_encode_dim,user_size):\n","\n","    input_length  = input_dim\n","    print(input_length)\n","    audio_encode_dim = audio_encode_dim\n","    user_size = user_size\n","    \n","    #------------------------------------------AUDIO MODEL-------------------------------------------\n","    audio_input = keras.layers.Input(shape=(input_length),name=\"audio_input\")\n","   \n","    #flatten\n","    dense1 = keras.layers.Dense(units = 88 , activation = \"tanh\")(audio_input)\n","    dense2 = keras.layers.Dense(units = 88 , activation = \"tanh\")(dense1)\n","    dense3 = keras.layers.Dense(units = 88 , activation = \"tanh\")(dense2)\n","    \n","    audio_model = keras.models.Model(inputs = audio_input , outputs = dense3)\n","    \n","    #-----------------------------------------USER MODEL------------------------------------------------------\n","    \n","    user1 = keras.layers.Input(shape=(user_size,) , name = \"User_Input\")\n","    user1_norm =  keras.layers.BatchNormalization()(user1)\n","    user_dense= keras.layers.Dense(units= int(user_size*2/3), activation=\"tanh\")(user1_norm)\n","    \n","    user_model = keras.models.Model(inputs = user1 , outputs = user_dense)\n","        \n","    #-----------------------------------------Concatination--------------------------------------------------\n","    combined = keras.layers.concatenate([audio_model.output, user_model.output])\n","    combined_norm = keras.layers.BatchNormalization()(combined)\n","    concat_dense1 = keras.layers.Dense(units= int((audio_encode_dim + user_size)/2), activation=\"tanh\")(combined_norm)\n","    concat_dense2 = keras.layers.Dense(units=1024, activation=\"sigmoid\")(concat_dense1)    \n","    concat_dense3 = keras.layers.Dense(units=128, activation=\"sigmoid\" )(concat_dense2)    \n","    concat_linear = keras.layers.Dense(units=1, activation=\"sigmoid\")(concat_dense3)\n","    \n","    model = keras.models.Model(inputs=[audio_model.input, user_model.input], outputs = concat_linear)\n","    \n","    return model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"p04SZU9BJ4ha"},"outputs":[],"source":["model = audio_Main_Model(input_dimension,audio_encode_dim,user_size)\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3Q0o4sHpJ4hb"},"outputs":[],"source":["print(model.summary())"]},{"cell_type":"markdown","metadata":{"id":"gV6o1kPrJ4hb"},"source":["# Training "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kgYEfW8KJ4hb"},"outputs":[],"source":["users_train, movies_train, nrates_train = get_nth_batch(train_obj)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"O4225D3MJ4hb"},"outputs":[],"source":["from tensorflow.keras.callbacks import EarlyStopping\n","early_stopping_callback = EarlyStopping(monitor = 'loss', patience = 12, mode = 'min', restore_best_weights = True)\n"," \n","# Compile the model and specify loss function, optimizer and metrics to the model.\n","model.compile(loss='mse', optimizer='RMSprop', metrics=['mean_squared_error'])\n","\n","# model_training_history = model.fit(x = [movies_train,users_train], y = nrates_train, epochs = 100, batch_size = 16 , shuffle = True, validation_split=0.1) \n","# Start training the model.\n","model_training_history = model.fit(x = [movies_train,users_train], y = nrates_train, epochs = 100, batch_size = 16 , shuffle = True, validation_split=0.1, callbacks = [early_stopping_callback])"]},{"cell_type":"markdown","metadata":{"id":"pTl6zq2PJ4hc"},"source":["# Testing"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5CN-fWAZJ4hc"},"outputs":[],"source":["users_test, movies_test, nrates_test = get_nth_batch(test_obj)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iQ-vn5XOJ4hc"},"outputs":[],"source":["model_evaluation_history = model.evaluate(x = [movies_test,users_test], y = nrates_test)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CPJ7U8CWX3Uf"},"outputs":[],"source":["model.summary()"]},{"cell_type":"markdown","metadata":{"id":"EI6tO7-3Xn6G"},"source":["# Embedding Extraction"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"I8PnM6pqXr7N"},"outputs":[],"source":["audio_features_model = tf.keras.models.Model(inputs=model.get_layer(name = \"audio_input\").input,\n","                      outputs=model.get_layer(name = \"dense_2\").output,)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"A_ON9PAZYTLZ"},"outputs":[],"source":["audio.keys()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Qh2ErhmOYfz9"},"outputs":[],"source":["inputx = np.expand_dims(audio['tt0064086'], axis=0)\n","b = audio_features_model(inputx)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QmpsAAwAZQrv"},"outputs":[],"source":["l = list(audio.keys())\n","arrx = np.zeros([len(l),89])\n","arrx = pd.DataFrame(arrx)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BLSNIaqGZV-q"},"outputs":[],"source":["for i in range(len(l)):\n","    x = l[i]\n","    inputx = np.expand_dims(audio[x], axis=0)\n","    outputx = audio_features_model(inputx)\n","    y = np.array(outputx) \n","    arrx.iloc[i,0] =  x\n","    arrx.iloc[i,1:] = y[0]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LJGnkL0raNV6"},"outputs":[],"source":["arrx"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"X0ac820aaX6T"},"outputs":[],"source":["arrx.to_csv('audio_embeddings_flickscore.csv' , index=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3TXogoPiavvf"},"outputs":[],"source":["axx_np = np.array(arrx)\n","embeddings_dic = {}\n","for i in range(len(axx_np)):\n","    embeddings_dic[axx_np[i][0]] = list(axx_np[i][1:])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_P14Ac1zbGBR"},"outputs":[],"source":["import pickle \n","file_pi = open('/content/drive/MyDrive/sony_IITPatna/dict_audio_embedding_fickscore.obj', 'wb') \n","pickle.dump(embeddings_dic, file_pi)"]}],"metadata":{"colab":{"collapsed_sections":["eoP5Mjtyzp3P","NASBYk9BzuTN","YhGlHmdpz5TZ","gV6o1kPrJ4hb","pTl6zq2PJ4hc","EI6tO7-3Xn6G"],"provenance":[{"file_id":"1npObdYu3J8d3gH2ed8YuP2JI4a2nkP2L","timestamp":1669989949072}]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.6"}},"nbformat":4,"nbformat_minor":0}