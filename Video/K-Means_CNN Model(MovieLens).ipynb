{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eoP5Mjtyzp3P"
   },
   "source": [
    "# Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WApHUHGBxZTM",
    "outputId": "4e2ba77b-161f-4b3f-9862-9c78f50670a5"
   },
   "outputs": [],
   "source": [
    "!pip install pickle5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "y0Hd9fVtxfSP",
    "outputId": "e5fd36d2-15fa-4d94-9dd1-b9157efd5e9e"
   },
   "outputs": [],
   "source": [
    "!pip install keras_applications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "hiNOmIuByOv-"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle5 as pickle\n",
    "from pathlib import Path\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "import string\n",
    "from math import ceil\n",
    "import random\n",
    "import numpy as np\n",
    "import sys\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import mean_squared_error as mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "VvCHHB1YzkXa"
   },
   "outputs": [],
   "source": [
    "def path_of(location):\n",
    "    me_dir, me_file= os.path.split(os.path.abspath(__file__))\n",
    "    return os.path.join(me_dir, location)\n",
    "\n",
    "def load_pkl(filename):\n",
    "    filename= filename\n",
    "    data= None\n",
    "    with open(filename, \"rb\") as handle:\n",
    "        data= pickle.load(handle)\n",
    "        handle.close()\n",
    "    return data\n",
    "\n",
    "def store_pkl(object, filename):\n",
    "    filename= filename\n",
    "    with open(filename, \"wb\") as handle:\n",
    "        pickle.dump(object, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        handle.close()\n",
    "\n",
    "def is_valid_file(filename):\n",
    "    filename= path_of(filename)\n",
    "    file= Path(filename)\n",
    "    if file.is_file():\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "\n",
    "def to_same_shape(arr_of_items, required_shape):\n",
    "    if len(arr_of_items) == 0:\n",
    "        print(\"Error: tried to make shape: \", required_shape, \", but item is empty...\")\n",
    "        exit()\n",
    "    if len(arr_of_items)>required_shape:\n",
    "        return arr_of_items[:required_shape]\n",
    "    res= []\n",
    "    ind=0\n",
    "    while len(res)<required_shape:\n",
    "        res.append(arr_of_items[ind])\n",
    "        ind= (ind+1)%len(arr_of_items)\n",
    "    return res\n",
    "\n",
    "def get_rand_str(size):\n",
    "    return \"\".join(random.choice(string.ascii_uppercase + string.digits) for _ in range(size))\n",
    "\n",
    "def save_model(keras_model, save_folder=\"./models\", save_filename= None):\n",
    "    if save_filename is None:\n",
    "        me_dir, me_file= os.path.split(os.path.abspath(__file__))\n",
    "        save_filename= me_file.split(\".\")[0]+\".h5\"\n",
    "    op_file= path_of(save_folder +\"/\"+save_filename)\n",
    "    keras_model.save(op_file)\n",
    "    print(\"\\n\\n    Saved_model:\", save_filename, \"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NASBYk9BzuTN"
   },
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YhGlHmdpz5TZ"
   },
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "1t5DdjG70AIx"
   },
   "outputs": [],
   "source": [
    "use_precomputed_dict_for_video= False\n",
    "\n",
    "imageW, imageH= 112, 112\n",
    "\n",
    "batch_size= 64\n",
    "\n",
    "train_obj= load_pkl(\"/home/pulkit_2111mt05/Flickscore_final/Dataset/train_v2.obj\") #will load pkl data using func\n",
    "test_obj= load_pkl(\"/home/pulkit_2111mt05/Flickscore_final/Dataset/test_v2.obj\")   #will load pkl data using func\n",
    "\n",
    "\n",
    "user_enc= load_pkl(\"/home/pulkit_2111mt05/Flickscore_final/Dataset/new_user_embedding_v2_500.obj\")     #will load pkl data using func\n",
    "user_size= len(user_enc.get(list(user_enc.keys())[0])) #user_size --> 915\n",
    "#print(user_size)\n",
    "\n",
    "\n",
    "\n",
    "video_frames_path= \"/home/pulkit_2111mt05/Flickscore_final/Dataset/Frames_entropy/\"\n",
    "\n",
    "def get_pixels(img_file:str):\n",
    "    im= Image.open(img_file, 'r')\n",
    "    im= im.resize((imageW, imageH, ))\n",
    "    pixel_values= im.getdata()\n",
    "    data= np.array(pixel_values, dtype=int).reshape((imageH, imageW, 3))\n",
    "    def range_squish(x): return x/255 #Normalization of image\n",
    "    data= range_squish(data)\n",
    "    return data      #Return pixel array of an image H*W*3\n",
    "\n",
    "\n",
    "def make_vec_from_video_folder(folder_path:str):\n",
    "    #print(folder_path)\n",
    "    if not os.path.isdir(folder_path):\n",
    "        # print(\"Invalid folder:\", folder_path)\n",
    "        raise Exception(\"Invalid folder path\")\n",
    "        print(folder_path)\n",
    "    files= [folder_path+\"/\"+f for f in os.listdir(folder_path) if os.path.isfile(os.path.join(folder_path, f))]\n",
    "    \n",
    "    pixels_marged= None\n",
    "    for f in files:\n",
    "        pixels_for_this_frame= get_pixels(f)\n",
    "        if pixels_marged is None:\n",
    "            pixels_marged= pixels_for_this_frame\n",
    "        else:\n",
    "            pixels_marged= np.concatenate([pixels_marged, pixels_for_this_frame], axis=-1)\n",
    "        \n",
    "    return pixels_marged\n",
    "\n",
    "\n",
    "def shuffle_single_epoch(ratings):  #will shuffle the provided data\n",
    "    data_copied= ratings.copy() \n",
    "    random.shuffle(data_copied)\n",
    "    return data_copied\n",
    "\n",
    "    \n",
    "def normalize_rate(rate):\n",
    "    return rate/5\n",
    "\n",
    "def de_normalize_rate(rate):\n",
    "    return rate*5\n",
    "\n",
    "\n",
    "def get_nth_batch(data):  \n",
    "    \n",
    "    users, movies, nrates= [], [], [] # 3 empty list \n",
    "    \n",
    "    for user_id, movie_id, rate in data:\n",
    "        #user_id --> 11megha89 , movie_id --.tt014651 , rate --> 1,0,-1\n",
    "        if user_enc.get(user_id) is None:   #Return user_vec --> 915            \n",
    "            continue\n",
    "        try:\n",
    "            m_vid= make_vec_from_video_folder(video_frames_path+str(movie_id))\n",
    "            #print(m_vid.shape) #-->(112, 112, 9)\n",
    "        except:\n",
    "            continue\n",
    "        \n",
    "        users.append(user_enc.get(user_id)) #Will \n",
    "        movies.append(m_vid)\n",
    "        if int(rate) == -1:\n",
    "               nrates.append([0,0,1])\n",
    "        elif int(rate) == 1:\n",
    "               nrates.append([0,1,0])\n",
    "        elif int(rate) == 0:\n",
    "               nrates.append([1,0,0])\n",
    "               \n",
    "    users= np.array(users, dtype=float)\n",
    "    # print(users.shape) --> (64, 915)\n",
    "    movies= np.array(movies, dtype=float)\n",
    "    # print(movies.shape) --> (64, 112, 112, 9)\n",
    "    nrates= np.array(nrates, dtype=float)\n",
    "    #print(nrates.shape) --> (64,)\n",
    "\n",
    "    assert len(users)==len(movies)==len(nrates)\n",
    "\n",
    "    return users, movies, nrates\n",
    "\n",
    "\n",
    "\n",
    "def test(model, test_data):\n",
    "    test_acc,test_loss= [],[]\n",
    "    test_final_acc,test_final_loss = [] , []\n",
    "    batch_count= ceil(len(test_data)/batch_size)\n",
    "    for batch_id in range(batch_count):\n",
    "        print(\" -> Testing Batch: \", batch_id)\n",
    "        user, movie, rate= get_nth_batch(test_data, batch_id)\n",
    "        loss , acc = model.evaluate([movie, user], rate)\n",
    "        test_acc.append(acc)\n",
    "        test_loss.append(loss)\n",
    "        \n",
    "    test_final_acc.append(sum(test_acc)/len(test_acc))\n",
    "    test_final_loss.append(sum(test_loss)/len(test_acc))\n",
    "    return sum(test_acc)/len(test_acc) , sum(test_loss)/len(test_acc)\n",
    "\n",
    "\n",
    "\n",
    "def train(model, data, test_data= None, epochs=50):\n",
    "    test_acc_main = 0\n",
    "    batch_count= ceil(len(data)/batch_size)       \n",
    "    for epoch_id in range(1, epochs+1):\n",
    "        data= shuffle_single_epoch(data)\n",
    "        print(\"\\n\\t---- Starting Epoch:\", epoch_id, \"----\")\n",
    "        \n",
    "        for batch_id in range(batch_count):\n",
    "            print(\" -> Batch: \", batch_id)\n",
    "            print(batch_id)\n",
    "            user, movie, rate= get_nth_batch(data, batch_id,batch_size) \n",
    "            #print(rate.shape)                        \n",
    "            model.fit([movie, user], rate, batch_size=batch_size, epochs=1)            \n",
    "            \n",
    "        if test_data is not None:\n",
    "            test_acc , test_loss= test(model, test_data)\n",
    "            print(\"TestAcc after Epoch\",epoch_id,\": \",test_acc)\n",
    "            print(\"TestLoss after Epoch\",epoch_id,\": \",test_loss)\n",
    "            if test_acc>test_acc_main:\n",
    "                    test_acc_main = test_acc\n",
    "                    location= \"./model_vid/\"\n",
    "                    model.save(location+\"/model_abc.h5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data_obj = test_obj + train_obj\n",
    "pd_data = pd.DataFrame(data_obj)\n",
    "\n",
    "results = []\n",
    "for i in pd_data.iloc[:,2]:\n",
    "    results.append(i[0])\n",
    "\n",
    "pd_data[3] = results\n",
    "x_data = np.array(pd_data.iloc[:,:2])\n",
    "y_data = np.array(pd_data.iloc[:,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(x_data, y_data, stratify=y_data ,test_size=0.2)\n",
    "\n",
    "train_obj = pd.DataFrame(X_train)\n",
    "train_obj[3] = pd.DataFrame(y_train)\n",
    "train_obj = np.array(train_obj)\n",
    "\n",
    "test_obj = pd.DataFrame(X_test)\n",
    "test_obj[3] = pd.DataFrame(y_test)\n",
    "test_obj = np.array(test_obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Main_Model(image_dim):\n",
    "    image_width , image_height , image_length  = image_dim\n",
    "    print(image_width , image_height , image_length)\n",
    "    cnn_encode_dim = 231\n",
    "    user_size = 915\n",
    "    \n",
    "    #------------------------------------------CNN MODEL-------------------------------------------\n",
    "    cnn_input = keras.layers.Input(shape=(image_width,image_height,image_length),name=\"Input_frames\")\n",
    "    #Conv1 \n",
    "    conv1 = keras.layers.Conv2D(4, kernel_size=(5, 5),strides=(1, 1), padding=\"same\")(cnn_input)\n",
    "    act_1 = keras.layers.Activation(keras.activations.relu)(conv1)\n",
    "    drop_1 = keras.layers.Dropout(rate=0.2)(act_1)\n",
    "    maxpool_1 = keras.layers.MaxPooling2D(pool_size=(2,2) , strides = (2,2) , padding = \"valid\")(drop_1)\n",
    "    norm_1 = keras.layers.BatchNormalization()(maxpool_1)\n",
    "    \n",
    "    #Conv2\n",
    "    conv2 = keras.layers.Conv2D(8, kernel_size=(5, 5),strides=(2, 2), padding=\"same\")(norm_1)\n",
    "    act_2 = keras.layers.Activation(keras.activations.relu)(conv2)\n",
    "    drop_2 = keras.layers.Dropout(rate=0.2)(act_2)    \n",
    "    norm_2 = keras.layers.BatchNormalization()(drop_2)\n",
    "\n",
    "    #Conv3\n",
    "    conv3 = keras.layers.Conv2D(8, kernel_size=(3, 3),strides=(2, 2), padding=\"same\")(norm_2)\n",
    "    act_3 = keras.layers.Activation(keras.activations.relu)(conv3)\n",
    "    norm_3 = keras.layers.BatchNormalization()(act_3)\n",
    "    \n",
    "    #flatten\n",
    "    flat = keras.layers.Flatten()(norm_3)\n",
    "    dense1 = keras.layers.Dense(units = 512 , activation = \"tanh\")(flat)\n",
    "    dense2 = keras.layers.Dense(units = 256 , activation = \"tanh\")(dense1)\n",
    "    dense3 = keras.layers.Dense(units = cnn_encode_dim , activation = \"tanh\" , name=\"cnn_feature\")(dense2)\n",
    "    \n",
    "    cnn_model = keras.models.Model(inputs = cnn_input , outputs = dense3)\n",
    "    \n",
    "    #-----------------------------------------USER MODEL------------------------------------------------------\n",
    "    \n",
    "    user1 = keras.layers.Input(shape=(user_size,) , name = \"User_Input\")\n",
    "    user1_norm =  keras.layers.BatchNormalization()(user1)\n",
    "    user_dense= keras.layers.Dense(units= int(user_size*2/3), activation=\"tanh\")(user1_norm)\n",
    "    \n",
    "    user_model = keras.models.Model(inputs = user1 , outputs = user_dense)\n",
    "        \n",
    "    #-----------------------------------------Concatination--------------------------------------------------\n",
    "    combined = keras.layers.concatenate([cnn_model.output, user_model.output])\n",
    "    combined_norm = keras.layers.BatchNormalization()(combined)\n",
    "    concat_dense1 = keras.layers.Dense(units= int((cnn_encode_dim + user_size)/2), activation=\"tanh\" )(combined_norm)\n",
    "    concat_dense2 = keras.layers.Dense(units=1024, activation=\"relu\")(concat_dense1)   \n",
    "    concat_dense2 = keras.layers.BatchNormalization()(concat_dense2)\n",
    "    concat_dense3 = keras.layers.Dense(units=128, activation=\"relu\")(concat_dense2)  \n",
    "    concat_dense3 = keras.layers.BatchNormalization()(concat_dense3)\n",
    "    concat_linear = keras.layers.Dense(units=1, activation=\"sigmoid\")(concat_dense3)\n",
    "    \n",
    "    model = keras.models.Model(inputs=[cnn_model.input, user_model.input], outputs = concat_linear)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "112 112 9\n",
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " Input_frames (InputLayer)      [(None, 112, 112, 9  0           []                               \n",
      "                                )]                                                                \n",
      "                                                                                                  \n",
      " conv2d (Conv2D)                (None, 112, 112, 4)  904         ['Input_frames[0][0]']           \n",
      "                                                                                                  \n",
      " activation (Activation)        (None, 112, 112, 4)  0           ['conv2d[0][0]']                 \n",
      "                                                                                                  \n",
      " dropout (Dropout)              (None, 112, 112, 4)  0           ['activation[0][0]']             \n",
      "                                                                                                  \n",
      " max_pooling2d (MaxPooling2D)   (None, 56, 56, 4)    0           ['dropout[0][0]']                \n",
      "                                                                                                  \n",
      " batch_normalization (BatchNorm  (None, 56, 56, 4)   16          ['max_pooling2d[0][0]']          \n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " conv2d_1 (Conv2D)              (None, 28, 28, 8)    808         ['batch_normalization[0][0]']    \n",
      "                                                                                                  \n",
      " activation_1 (Activation)      (None, 28, 28, 8)    0           ['conv2d_1[0][0]']               \n",
      "                                                                                                  \n",
      " dropout_1 (Dropout)            (None, 28, 28, 8)    0           ['activation_1[0][0]']           \n",
      "                                                                                                  \n",
      " batch_normalization_1 (BatchNo  (None, 28, 28, 8)   32          ['dropout_1[0][0]']              \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " conv2d_2 (Conv2D)              (None, 14, 14, 8)    584         ['batch_normalization_1[0][0]']  \n",
      "                                                                                                  \n",
      " activation_2 (Activation)      (None, 14, 14, 8)    0           ['conv2d_2[0][0]']               \n",
      "                                                                                                  \n",
      " batch_normalization_2 (BatchNo  (None, 14, 14, 8)   32          ['activation_2[0][0]']           \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " flatten (Flatten)              (None, 1568)         0           ['batch_normalization_2[0][0]']  \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 512)          803328      ['flatten[0][0]']                \n",
      "                                                                                                  \n",
      " User_Input (InputLayer)        [(None, 915)]        0           []                               \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 256)          131328      ['dense[0][0]']                  \n",
      "                                                                                                  \n",
      " batch_normalization_3 (BatchNo  (None, 915)         3660        ['User_Input[0][0]']             \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " cnn_feature (Dense)            (None, 231)          59367       ['dense_1[0][0]']                \n",
      "                                                                                                  \n",
      " dense_2 (Dense)                (None, 610)          558760      ['batch_normalization_3[0][0]']  \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, 841)          0           ['cnn_feature[0][0]',            \n",
      "                                                                  'dense_2[0][0]']                \n",
      "                                                                                                  \n",
      " batch_normalization_4 (BatchNo  (None, 841)         3364        ['concatenate[0][0]']            \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " dense_3 (Dense)                (None, 573)          482466      ['batch_normalization_4[0][0]']  \n",
      "                                                                                                  \n",
      " dense_4 (Dense)                (None, 1024)         587776      ['dense_3[0][0]']                \n",
      "                                                                                                  \n",
      " batch_normalization_5 (BatchNo  (None, 1024)        4096        ['dense_4[0][0]']                \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " dense_5 (Dense)                (None, 128)          131200      ['batch_normalization_5[0][0]']  \n",
      "                                                                                                  \n",
      " batch_normalization_6 (BatchNo  (None, 128)         512         ['dense_5[0][0]']                \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " dense_6 (Dense)                (None, 1)            129         ['batch_normalization_6[0][0]']  \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2,768,362\n",
      "Trainable params: 2,762,506\n",
      "Non-trainable params: 5,856\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = Main_Model([imageW, imageH, 9])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "users_train, movies_train, nrates_train = get_nth_batch(train_obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "early_stopping_callback = EarlyStopping(monitor = 'val_loss', patience = 12, mode = 'min', restore_best_weights = True)\n",
    " \n",
    "# Compile the model and specify loss function, optimizer and metrics to the model.\n",
    "model.compile(loss='mse', optimizer='RMSprop', metrics=['mean_squared_error'])\n",
    " \n",
    "# Start training the model.\n",
    "model_training_history = model.fit(x = [movies_train,users_train], y = nrates_train, epochs = 100, batch_size = 16 , shuffle = True, validation_split=0.1, callbacks = [early_stopping_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "users_test, movies_test, nrates_test = get_nth_batch(test_obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "105/105 [==============================] - 2s 15ms/step - loss: 0.6927 - accuracy: 0.7347\n"
     ]
    }
   ],
   "source": [
    "model_evaluation_history = model.evaluate(x = [movies_test,users_test], y = nrates_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "418/418 [==============================] - 8s 15ms/step - loss: 0.5139 - accuracy: 0.8027\n"
     ]
    }
   ],
   "source": [
    "model_evaluation_history = model.evaluate(x = [movies_train,users_train], y = nrates_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save_Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime as dt\n",
    "# Get the loss and accuracy from model_evaluation_history.\n",
    "model_evaluation_loss, model_evaluation_accuracy = model_evaluation_history\n",
    " \n",
    "# Define the string date format.\n",
    "# Get the current Date and Time in a DateTime Object.\n",
    "# Convert the DateTime object to string according to the style mentioned in date_time_format string.\n",
    "date_time_format = '%Y_%m_%d__%H_%M_%S'\n",
    "current_date_time_dt = dt.datetime.now()\n",
    "current_date_time_string = dt.datetime.strftime(current_date_time_dt, date_time_format)\n",
    " \n",
    "# Define a useful name for our model to make it easy for us while navigating through multiple saved models.\n",
    "model_file_name = f'_model_Video_Embeddings___Date_Time_{current_date_time_string}___Loss_{model_evaluation_loss}___Accuracy_{model_evaluation_accuracy}.h5'\n",
    " \n",
    "# Save your Model.\n",
    "location= \"/home/pulkit_2111mt05/MovieLens_final/Save/\"\n",
    "model.save(location+model_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "eoP5Mjtyzp3P",
    "NASBYk9BzuTN",
    "IEK-DWoCdZdF"
   ],
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
